Cyndi Operator
===========================================

This is an Openshift operator. The project's scaffolding was generated by the
[operator-sdk v0.19](https://v0-19-x.sdk.operatorframework.io/docs/golang/quickstart/). At a high level,
it manages the Kafka Connector and DB table for an application's Cyndi pipeline. This includes creating the connector
and DB table, validating the data is syndicated correctly, and automatically refreshing the pipeline when the data
becomes out of sync.

## Development
[This is general info on building and running the operator.](https://v0-19-x.sdk.operatorframework.io/docs/golang/quickstart/#build-and-run-the-operator)

I've been using
[CodeReady Containers](https://developers.redhat.com/products/codeready-containers/overview) as a Kubernetes cluster.
It requires a ton of RAM (over 20 GB on my machine).
[MiniKube](https://github.com/kubernetes/minikube/releases) is a less resource hungry option.
The rest of this document assumes CodeReady Containers.

### Setting up the development environment

1. Download an unpack [CodeReady Containers](https://developers.redhat.com/products/codeready-containers/overview)

1. Configure CRC to use 16G of memory
    ```
    ./crc config set memory 16384
    ```

1. Start CRC
    ```
    ./crc start
    ```

1. When prompted for a pull secret paste it (you obtained pull secret on step 1 when downloading CRC)

1. Log in to the cluster as kubeadmin (oc login -u kubeadmin -p ...)
   You'll find the exact command to use in the CRC startup log

1. Log in to https://quay.io/
   From Account settings download a kubernetes secret.
   This secret is used to pull quay.io/cloudservices images

1. Install the quay secret to the cluster
    ```
    oc apply -f <secret name>.yml
    ```

1. Link the secret to the default serviceaccount
    ```
    oc secrets link default <secret name> --for=pull
    ```

1. In the `dev` folder run
    ```
    ./run-admin.sh
    ```
   and wait for it to finish

### Useful commands

Create a host

```
KAFKA_BOOTSTRAP_SERVERS=192.168.130.11:$(oc get service my-cluster-kafka-external-bootstrap -n my-kafka-project -o=jsonpath='{.spec.ports[0].nodePort}{"\n"}') python utils/kafka_producer.py
```

List hosts

```
curl -H 'x-rh-identity: eyJpZGVudGl0eSI6eyJhY2NvdW50X251bWJlciI6IjAwMDAwMDEiLCAidHlwZSI6IlVzZXIifX0K' http://api.crc.testing:$(oc get service insights-inventory-public -n my-kafka-project -o=jsonpath='{.spec.ports[0].nodePort}{"\n"}')/api/inventory/v1/hosts
```

[The Strimzi operator,](https://strimzi.io/docs/operators/latest/quickstart.html) HBI, and an
Application DB needs to be installed in the cluster. The Application DB can just be an empty Postgres DB. Execute steps
3 and 4 of the
[onboarding process](https://platform-docs.cloud.paas.psi.redhat.com/backend/inventory.html#onboarding-process)
in the Application DB.

Create secrets for the HBI DB, App DB, and a config map for the Cyndi Operator. See the examples folder.

As mentioned in the quickstart, run `make generate` any time a change is made to `cyndipipeline_types.go`.
Similarly, run `make manifests` any time a change is made which needs to regenerate the CRD manifests. Finally, run
`make install` to install/update the CRD in the Kubernetes cluster.

I find it easiest to run the operator locally. This allows the use of a debugger. Use `make delve` to start the
operator in debug mode. Then connect to it with a debugger on port 2345. It can also be run locally with
`make run ENABLE_WEBHOOKS=false`.

After everything is running, create a new Custom Resource via
`kubectl apply -f config/samples/cyndi_v1beta1_cyndipipeline.yaml`. Then, the CR can be managed via Kubernetes commands
like normal.

## Typical Flows

These are high level descriptions of the steps taken within the reconcile loop during typical scenarios.

#### Create a new pipeline

1. Update the CR's status with the pipeline version (uses the current timestamp)
1. Create a new table in AppDB (e.g. inventory.hosts_v1_1597073300783716678)
1. Create a new Kafka Sink Connector pointing to the new table
1. Attempt to validate the data is in sync. Each time the validation fails, it will requeue the reconcile loop until
validation succeeds, or the retry limit is reached. If the retry limit is reached before validation succeeds, the
existing table and connector will be deleted, and the pipeline will be recreated starting at step 1.
1. After validation succeeds, the DB view (inventory.hosts) will be updated to point to the new table.
1. The reconcile loop will be re-queued to run periodically to validate the data is in sync.

#### Delete a pipeline
1. The Kafka Sink Connector is deleted automatically because it is a child of the Cyndi Pipeline
1. The table is deleted via the finalizer: finalizeCyndiPipeline

#### Refresh an out of sync pipeline
1. When the validation failure count reaches the configured limit, the pipeline will be refreshed.
1. This is triggered by setting Status.PreviousPipelineVersion = Status.PipelineVersion, Status.PipelineVersion = ""
1. The following execution of the reconcile loop will create a new pipeline version (table, connector)
1. Validation will happen the same as when creating a new pipeline
1. After the validation succeeds, the DB view will point to the new table, and the old pipeline (table, connector)
   is deleted
1. The reconcile loop will be re-queued to run periodically to validate the data is in sync.

#### Migrate an existing pipeline to a new schema
`TODO`

## Other Notes

#### Validation Criteria
`TODO`

#### Connector Config
`TODO`

#### Database Schema
`TODO`

#### Database Connection Details
`TODO`
