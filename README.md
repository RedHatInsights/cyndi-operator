Cyndi Operator
===========================================

This is an Openshift operator. The project's scaffolding was generated by the
[operator-sdk v0.19](https://v0-19-x.sdk.operatorframework.io/docs/golang/quickstart/). At a high level,
it manages the Kafka Connector and DB table for an application's Cyndi pipeline. This includes creating the connector
and DB table, validating the data is syndicated correctly, and automatically refreshing the pipeline when the data
becomes out of sync.

## General info

[The Strimzi operator,](https://strimzi.io/docs/operators/latest/quickstart.html) HBI, and an
Application DB needs to be installed in the cluster. The Application DB can just be an empty Postgres DB. Execute steps
3 and 4 of the
[onboarding process](https://platform-docs.cloud.paas.psi.redhat.com/backend/inventory.html#onboarding-process)
in the Application DB.

Create secrets for the HBI DB, App DB, and a config map for the Cyndi Operator. See the examples folder.

As mentioned in the quickstart, run `make generate` any time a change is made to `cyndipipeline_types.go`.
Similarly, run `make manifests` any time a change is made which needs to regenerate the CRD manifests. Finally, run
`make install` to install/update the CRD in the Kubernetes cluster.

I find it easiest to run the operator locally. This allows the use of a debugger. Use `make delve` to start the
operator in debug mode. Then connect to it with a debugger on port 2345. It can also be run locally with
`make run ENABLE_WEBHOOKS=false`.

After everything is running, create a new Custom Resource via
`kubectl apply -f config/samples/cyndi_v1beta1_cyndipipeline.yaml`. Then, the CR can be managed via Kubernetes commands
like normal.

## Development
[This is general info on building and running the operator.](https://v0-19-x.sdk.operatorframework.io/docs/golang/quickstart/#build-and-run-the-operator)

I've been using
[CodeReady Containers](https://developers.redhat.com/products/codeready-containers/overview) as a Kubernetes cluster.
It requires a ton of RAM (over 20 GB on my machine).
[MiniKube](https://github.com/kubernetes/minikube/releases) is a less resource hungry option.
The rest of this document assumes CodeReady Containers.

### Setting up the development environment

1. Download an unpack [CodeReady Containers](https://developers.redhat.com/products/codeready-containers/overview)

1. Append the following line into `/etc/hosts`
    ```
    127.0.0.1 advisor-db inventory-db
    ```

1. Configure CRC to use 16G of memory
    ```
    ./crc config set memory 16384
    ```

1. Start CRC
    ```
    ./crc start
    ```

1. When prompted for a pull secret paste it (you obtained pull secret on step 1 when downloading CRC)

1. Log in to the cluster as kubeadmin (oc login -u kubeadmin -p ...)
   You'll find the exact command to use in the CRC startup log

1. Create a `my-kafka-project` namespace
    ```
    oc create ns my-kafka-project
    ```

1. Log in to https://quay.io/
   From Account settings download a kubernetes secret.
   This secret is used to pull quay.io/cloudservices images

1. Install the quay secret to the cluster
    ```
    oc apply -n my-kafka-project -f <secret name>.yml
    ```

1. In the `dev` folder run
    ```
    ./run-admin.sh <secret name>
    ```
   and wait for it to finish

1. Set up port-forwarding to database pods
    ```
    oc port-forward svc/inventory-db 5432:5432 -n my-kafka-project &
    oc port-forward svc/advisor-db 5433:5432 -n my-kafka-project &
    ```

1. Install CRDs
    ```
    make install
    ```

1. Run the operator
    ```
    make run ENABLE_WEBHOOKS=false
    ```

1. Finally, create a new pipeline
    ```
    oc apply -f ../config/samples/cyndi_v1beta1_cyndipipeline.yaml
    ```

1.

### Useful commands

Create a host
```
KAFKA_BOOTSTRAP_SERVERS=192.168.130.11:$(oc get service my-cluster-kafka-external-bootstrap -n my-kafka-project -o=jsonpath='{.spec.ports[0].nodePort}{"\n"}') python utils/kafka_producer.py
```

List hosts
```
curl -H 'x-rh-identity: eyJpZGVudGl0eSI6eyJhY2NvdW50X251bWJlciI6IjAwMDAwMDEiLCAidHlwZSI6IlVzZXIifX0K' http://api.crc.testing:$(oc get service insights-inventory-public -n my-kafka-project -o=jsonpath='{.spec.ports[0].nodePort}{"\n"}')/api/inventory/v1/hosts
```

Inspect Kafka Connect cluster
```
oc port-forward svc/my-connect-cluster-connect-api 8083:8083 -n my-kafka-project
```
then access the kafka connect API at http://localhost:8083/connectors

Connect to inventory db
```
pgcli -h localhost -p 5432 -u insights insights
```

Connect to advisor db
```
pgcli -h localhost -p 5433 -u insights insights
```

## Typical Flows

These are high level descriptions of the steps taken within the reconcile loop during typical scenarios.

#### Create a new pipeline

1. Update the CR's status with the pipeline version (uses the current timestamp)
1. Create a new table in AppDB (e.g. inventory.hosts_v1_1597073300783716678)
1. Create a new Kafka Sink Connector pointing to the new table
1. Attempt to validate the data is in sync. Each time the validation fails, it will requeue the reconcile loop until
validation succeeds, or the retry limit is reached. If the retry limit is reached before validation succeeds, the
existing table and connector will be deleted, and the pipeline will be recreated starting at step 1.
1. After validation succeeds, the DB view (inventory.hosts) will be updated to point to the new table.
1. The reconcile loop will be re-queued to run periodically to validate the data is in sync.

#### Delete a pipeline
1. The Kafka Sink Connector is deleted automatically because it is a child of the Cyndi Pipeline
1. The table is deleted via the finalizer: finalizeCyndiPipeline

#### Refresh an out of sync pipeline
1. When the validation failure count reaches the configured limit, the pipeline will be refreshed.
1. This is triggered by setting Status.PreviousPipelineVersion = Status.PipelineVersion, Status.PipelineVersion = ""
1. The following execution of the reconcile loop will create a new pipeline version (table, connector)
1. Validation will happen the same as when creating a new pipeline
1. After the validation succeeds, the DB view will point to the new table, and the old pipeline (table, connector)
   is deleted
1. The reconcile loop will be re-queued to run periodically to validate the data is in sync.

#### Migrate an existing pipeline to a new schema
`TODO`

## Other Notes

#### Validation Criteria
`TODO`

#### Connector Config
`TODO`

#### Database Schema
`TODO`

#### Database Connection Details
`TODO`
